# An explanation of how these models work

The flow of the model is quite straightforward, in that it's quite linear, but it goes through a few phases.

The entry point to a model is generally `k_fold_by_players`, which takes a few arguments, the most important being `iterations`, `games`, `fold_count`, and a `correlation_metric`. I'll explain how these are used as they come up.

In `k_fold_by_players`, we generate many folds of the list of (mongobd-sourced) `games`. We then perform a grid search against these folds. This is to say that the whole experiment is looking for a float in [0,1], so we check the fitness of the model with 0.0, 0.1, 0.2...0.9 as arguments by minimising the return value of the `correlation_metric` argument (a callable) with the simulated games produced at the given rgr values (and real-world games to correlate against) as arguments. 

The function we minimise is `mitigate_randomness`, but what we're really interested in is the argument we pass into it, which is `compare_with_multiple_players`. The two get run via a convoluted nested list structure passed into a function called `parallelisable_with_random_seed` --- this is to easily batch jobs if we move to a concurrent execution model. It's a hangover from previous iterations but I've kept it in Just In Case because it works right now anyway.

_Anyway_: `mitigate_randomness` runs the function passed into it many times and averages out the result. We're passing in `compare_with_multiple_players`, which returns a set of character pair choices. 

`compare_with_multiple_players` returns character pair choices as generated by a function it runs, `generate_synthetic_data`, which is doing the bulk of the work of the experiment. It applies aspects to our model and runs it a parameterisable number of times, returning the results collected/observed by the aspects.

At the end of the experimental execution, `compare_with_multiple_players` normalises the results, which it then returns.

`mitigate_randomness` collects multiple of these return values and averages them out. It returns the experimental results averaged over these multiple iterations.

k_fold_by_players uses the return value of this as an argument for its `correlation_metric`, as mentioned above. It runs the above process for a set of RGRs, finds the correlation metric output, and selects the best it can find. It then _refines_ the RGR which gave the best ( _minimal_ ) value from the correlation metric, by searching in increasingly small increments around the best values it finds. This is implemented right now as searching around the best decimal point so far: so, at first, it finds the best value at 0.x; once it's found that, it searches at the 0.x it found to find the best slightly-refined value 0.xy (where y might be negative, i.e. it searches above and below 0.x). It continues for a given number of decimal points as defined by its `depth` parameter.


## The aspects we apply

There's a few aspects being applied — they're all in the `tom_models.aspects` package.

1. `handle_player_cannot_win` is necessary because William's lookup tables for optimal moves end in the middle of a game if that game _cannot_ be won by either player. In the case where this error is raised, this aspect intercepts the exception rising through the stack and records the appropriate win/loss for the player that cannot lose/win.
2. `record_player_sees_winning_team` keeps a list of the charpairs each player has seen win a game at the end of each game.
3. `record_simulated_choices` maintains a list of the charpair choices made by each simulated player
4. `around_choosing_chars_based_on_sigmoid` intercepts character pair choices and will replace the choice with our our own artificial choice if our model dictates that this should happen. To do this, it also uses the function `tom_models.aspects.update_confidence_model`, which keeps track of the confidence of each player as defined by a parameterised sigmoid using the variables passed to `tom_models.experiment_base.generate_synthetic_data` — by default a logistic curve with a parameterised rgr.
5. `best_move_generator` _returns_ an aspect for fixing the move each player makes as the best possible one, as defined by William's lookup tables of optimal moves. THe generator function is necessary because the function applies to one where the experimental environment isn't passed as an argument, so we pass it into `best_move_generator`, thereby keeping it in the _scope_ of the generated aspect.


## Processing games

Database interaction is performed via `shepherd`, a wrapper which caches results locally and provides a bunch of options for filtering based on games, users, and devs/beta testers depending on the data we want to see. The local cache is helpful because it allows us to work offline / without being connected to the actual DB. There's a bit in here but it's really just wrappers around shepherd and not that important — I'll document the functions properly, but they basically do what they say on the tin, and anything exotic has a docstring.


## Correlation measurement functions

useful functions for measuring distance for k-fold validation and the like; I've kept them because they took a while to figure out at first and I might need them in the future, but right now, I don't make use of these.
